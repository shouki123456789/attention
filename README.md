# Attention

AI to predict a masked word in a text sequence.

This project demonstrates how to use BERT, a transformer-based model, for predicting masked words in a sentence and visualizing the attention mechanisms within BERT's layers and heads.

## Introduction

This project uses the BERT model to predict masked words in sentences and visualize the attention patterns across different layers and heads.
BERT uses a transformer architecture and therefore uses an attention mechanism for understanding language. 
By analyzing the attention diagrams, you can infer which relationships the model has learned and understand the decision-making process behind the predictions.

## Features

* **Masked Word Prediction**: Predicts the most likely words to fill in a masked token in a given sentence.
* **Attention Visualization**: Generates diagrams that illustrate how attention is distributed across tokens in different layers and heads of the BERT model.

## Specification

For more information about the project specifications, please refer to the [Harvard CS50AI](https://cs50.harvard.edu/ai/) website.
Please avoid directly copying the source code as it is provided for reference purposes only.

